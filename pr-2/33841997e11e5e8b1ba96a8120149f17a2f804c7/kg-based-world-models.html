<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    <title>KG-based world models</title>
    <meta name="description" content="Ammanabrolu and Riedel frame a Set of Sequences (SOS) generation problem. Contribute loss function + Worldformer.">
    <link rel="canonical" href="https://tkukurin.github.io/kg-based-world-models">
  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/skins/light.css">
  <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

</head>


  <body class="layout--post  kg-based-world-models">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/">~</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">caveat emptor</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <div class="page-sidebar">
        
            
            <h1 id="page-title" class="page-title p-name">KG-based world models
</h1>
        
        <div class="page-author h-card p-author"><div class="author-info">
    <time class="page-date dt-published" datetime="2021-10-05T00:00:00+00:00"><a class="u-url" href="">Oct 5, 2021</a>
</time>

  </div>
</div>

        

        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy">rl</li><li class="page-taxonomy">nlp</li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <p><a href="https://arxiv.org/pdf/2106.09608.pdf">Ammanabrolu and Riedel</a> frame a Set of
Sequences (SOS) generation problem. Contribute loss function + <em>Worldformer</em>.</p>

<blockquote>
  <p>we hypothesize that agents that rely on lifted representations of the world
will benefit from more than just memorization but the ability to predict how
the graph state representation will change. For example, by inferring that a
locked chest is likely to contain treasure before it is actually revealed
provides an agent with a form of look-ahead that will potentially enable it to
bias its actions towards opening such a chest.</p>
</blockquote>

<p>Conclusions seem to be:</p>
<ol>
  <li>Predicting KG differences between states makes the problem more tractable;</li>
  <li>Multi-task training improvements: acting &amp; mapping in Jericho games is highly
correlated, thus benefits from joint solving;</li>
  <li>Improvements from SOS loss: account for graph properties.</li>
</ol>

<h2 id="dataset-jerichoworld">Dataset: <a href="https://github.com/JerichoWorld/JerichoWorld">JerichoWorld</a></h2>

<p>27 text games for 24,198 mappings between NL observations and</p>
<ol>
  <li>KGs as \((s, r, o)\) tuples reflecting world state.</li>
  <li>NL actions that guarantee to cause changes in a particular world state.</li>
</ol>

<p>They use a random 10% of the training set for validation.
Further 7,836 heldout instances over 9 additional games in the test set.</p>

<p>Not traditional RL, instead <em>outputs</em> for the 2 tasks are:</p>
<ol>
  <li>From current observations and graph, predict some/all of the next KG.</li>
  <li>From current observations and graph, predict some/all of the valid actions.</li>
</ol>

<p><img src="/assets/img/2021-10-09-jerichoworld-zorkl.png" alt="JerichoWorld Zorkl graph example" /></p>

<p>Example input:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Game: ztuu
Location: Cultural Complex This imposing ante-room, the center of (...).
North of here, a large hallway passes (...). To the east, (...). South, (...).
You can see a pair of razor-like gloves here.
Observation: You put on the razor-like gloves.
Inventory:
  You are carrying:
    a brass lantern (providing light), (...)
    four candy bars:
      a ZM$100000
      a Multi-Implementeers (...)
Prev Act: put on gloves
Inventory Objects:
  candy: Which do you mean, the ZM$100000, the Multi Implementeers, (...)
  Implementeers: The profiles on the wrapper of this delicacy look (...)
  sword: This is a cheaply made sword of no antiquity whatsoever (...)
  rune: The label is covered with mystical runes (...)
Inventory Attributes:
  glasses: clothing
  gloves: clothing
  sword: animate, equip
  lantern: animate, equip
Surrounding Objects:
  gargoyles: Unless you are inordinately masochistic, the less time (...)
  east: You see nothing special about the east wall.
  tunnel: The tunnel leads west.
  gloves: (...) would be very attractive for an axe murderer (...)
  south: You see nothing special about the south wall.
  sign: The sign indicates today’s performance (...)
Surrounding Attributes:
  gloves: clothing
  tunnel: animate
  sign: animate
Graph:
  [sign, in, CC], [you, have, sword], [tunnel, in, CC], [you, in, CC],
  [glasses, is, clothing], [gloves, is, clothing], [lantern, is, equip] (...)
Valid Actions: west, turn lantern off, east, south, (...)
</code></pre></div></div>

<h2 id="baselines">Baselines</h2>

<ul>
  <li>OpenIE based on Ammanabrolu’s previous work.</li>
  <li>Q*BERT: Question-Answering ALBERT, e.g. “What is my current location?”
Trained on SQuAD 2.0 and JerichoQA.</li>
  <li>Seq2Seq: Bidirectional BERT encoder for observation+graph. Autoregressive
GPT-2 decoder to decode the next graph. Trained using cross-entropy loss.</li>
  <li>GATA-World predicts <code class="language-plaintext highlighter-rouge">(add|del, h, r, t)</code> using single-task CE loss.</li>
</ul>

<h2 id="model-worldformer">Model: Worldformer</h2>

<p>Multi-task: knowledge graph + action generation.
Basically two transformer encoders w/ different objectives. Sample inputs:</p>
<ol>
  <li><code class="language-plaintext highlighter-rouge">[OBS] West of house (...) [ACT] go north [ACT] open mailbox (...)</code></li>
  <li><code class="language-plaintext highlighter-rouge">[GRAPH] you, in, West of house
 [TRIPLE] West of house, has, mailbox
 [TRIPLE] mailbox, is, openable (...)</code></li>
</ol>

<p>Pooled embeddings from the 2 encoders are concatenated.
Decoder also (of course!) attends to individual token vectors.</p>

<p><img src="assets/img/2021-10-09-worldformer.png" alt="Worldformer diagram" /></p>

<p>Outputs, KG difference \(G_{t+1} − G_t\) and the valid actions \(V_{t+1}\),
are both <em>Sets of Sequences</em>: token ordering matters within, but not between.
Usual loss is standard max-likelihood:</p>

\[L_{\mathrm{seq}} = \log \mathbb P(Y|X) =
  \sum_i^{M+1}\log \mathbb P(y_i | y_{0:i-1}), X)\]

<p>Their set-of-sequences loss groups tokens together.</p>

\[Y_{\mathrm{sos}} =
  \{y_1'\ldots y_N'\}, y_i' \in V_{t+1} \cup (G_{t+1} - G_t) \\

y_i' = \{y_k, \ldots y_{k+l}\}, \sum_j \mathrm{len}(y_j') = M \\

L_{\mathrm{sos}} =
  \sum_j^{M+1}
  \sum_{k=l}^{l+n}
    \log p(y_k | y_{l:k-1}), X; \theta) \\

\mathrm{where}\ l = \sum_{j&lt;i} \mathrm{len}(y_j'), n = \mathrm{len}(y_i')\]

<p>Finally, we can infer \(G_t - G_{t+1}\) due to some JerichoWorld invariants:</p>
<ol>
  <li>Locations are fixed and unique (their relative positions don’t change)</li>
  <li>Objects and characters can only be in one location at a time;</li>
  <li>Contradicting attributes can be discarded (e.g. WordNet: <code class="language-plaintext highlighter-rouge">!(open &amp; closed)</code>)</li>
</ol>

<p>Hence this output is not predicted by their decoder.</p>

<h2 id="methodology">Methodology</h2>

<p>Encoders similar to BERT, decoders similar to GPT-2.
Trained until validation acc does not improve for 5 epochs or 96 hours (4
GeForce RTX 2080 GPUs), 3 random seeds.
Decoding using beam search with a beam width of 15 at test time until EOS.</p>

<p>Vocabulary for the action decoder is 11,056 and 7,002 for the graph decoder.
The graph decoder vocab contains <em>all entities and relations</em> from train+test.
Full set of hyperparameters <a href="https://arxiv.org/pdf/2106.09608.pdf">on the last page, Table 5</a>.</p>

<blockquote>
  <p>JerichoWorld devs note that there is a correlation between performance of the
baseline Seq2Seq model to the average number of valid actions for the testing
game. [Likely due to dataset imbalance; the model likely learns] a common set
of actions found across all games [e.g. navigation] before fine-grained ones.</p>
</blockquote>


        </div>

        

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/kim-stachenfeld">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Learning salon: Kimberly Stachenfeld

      </span>
    </a>
  

  
    <a class="page-next" href="/probing-emergent-semantics-qa">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        Probing Emergent Semantics in Predictive Agents via Question Answering
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>

    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://twitter.com/tkukurin"><i class="fab fa-twitter-square fa-2x" title="Twitter"></i></a><a class="social-icon" href="https://github.com/tkukurin"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/toni/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a></div><div class="copyright">
    
      

    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>

</html>
