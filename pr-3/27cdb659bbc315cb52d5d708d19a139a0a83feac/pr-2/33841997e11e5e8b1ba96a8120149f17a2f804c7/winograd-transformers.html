<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    <title>Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema</title>
    <meta name="description" content="From Elazar et al.. Evaluated on 2 Winograd corpora (“The X doesn’t fit into Y because it is Z”): Winograd Schema Challenge (WSC, 2012) 273 manually curated ...">
    <link rel="canonical" href="https://tkukurin.github.io/winograd-transformers">
  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/skins/light.css">
  <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

</head>


  <body class="layout--post  back-to-square-one-artifact-detection-training-and-commonsense-disentanglement-in-the-winograd-schema">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/">~</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">caveat emptor</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <div class="page-sidebar">
        
            
            <h1 id="page-title" class="page-title p-name">Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema
</h1>
        
        <div class="page-author h-card p-author"><div class="author-info">
    <time class="page-date dt-published" datetime="2021-10-14T00:00:00+00:00"><a class="u-url" href="">Oct 14, 2021</a>
</time>

  </div>
</div>

        

        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy">nlp</li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <p><a href="https://arxiv.org/pdf/2104.08161.pdf">From Elazar et al.</a>.
Evaluated on 2 Winograd corpora (“The X doesn’t fit into Y because it is Z”):</p>
<ul>
  <li><a href="https://www.aaai.org/ocs/index.php/KR/KR12/paper/viewPaper/4492">Winograd Schema Challenge (WSC, 2012)</a>
273 manually curated to minimize influence of priors; e.g. <em>The racecar zoomed
by the school bus because it was going so fast</em> (racecars are usually fast).</li>
  <li><a href="https://arxiv.org/abs/1907.10641">WinoGrande (2019)</a>, adversarial Winograd
w/ a bunch of train (40k / 9k unbiased), dev (1.2k), test (1.7k) samples.
    <blockquote>
      <p>The best state-of-the-art methods on WinoGrande achieve 59.4-79.1%, which are
15-35% below human performance of 94.0%</p>
    </blockquote>
  </li>
</ul>

<p>Twin sentences in the context of WS:</p>
<ol>
  <li>The trophy does not fit into the brown suitcase because it is too <strong>small</strong>;</li>
  <li>The trophy does not fit into the brown suitcase because it is too <strong>large</strong></li>
</ol>

<p>Their explanations for the (falsely?) perceived progress on the WS task:</p>
<ol>
  <li>lax evaluation criteria; partially alleviate it via <em>group-scoring</em>: model
credited with the worst performing score of a group
    <blockquote>
      <p>For WS, where the task involves a binary classification, we use group scoring
over the twin sentences, with accuracy as the per-instance scoring function.</p>
    </blockquote>
  </li>
  <li>remaining artifacts in the datasets; alleviate via baseline ctxs (below)</li>
  <li>knowledge and reasoning leakage from large training data; don’t use PLMs (?)
    <blockquote>
      <p>we claim that the vast majority of commonsense knowledge a model obtains
should come from sources external to the supervised dataset […] The
supervised training set should mainly provide a means for learning the
format of the task but not as a source for commonsense knowledge acquisition.</p>
    </blockquote>
  </li>
</ol>

<p>Winograd transformers usually take argmax after replacing <em>it</em> in schemas with
corresponding entities. They test BERT, RoBERTa and ALBERT (<code class="language-plaintext highlighter-rouge">[CLS] context [SEP]
entity [SEP]</code>; entity is e.g. axe/suitcase; output is binary, from <code class="language-plaintext highlighter-rouge">[CLS]</code>):</p>
<ol>
  <li>Full ctx: <em>The axe would not fit in the suitcase because it’s too large.</em></li>
  <li>Baseline, no-candidate ctx: <em>would not fit in because it’s too large.</em></li>
  <li>Baseline, partial sent ctx: <em>because it’s too large.</em></li>
</ol>

<blockquote>
  <p>We note that these two baselines create nonsensical sentences. Therefore, we
expect humans to not be able to properly solve them. Thus, a model that
achieves higher than random performance on these baselines over a large enough
dataset is suspected to rely on spurious correlations.</p>
</blockquote>

<p>I mean… Okay, but humans instructed to use their priors could actually
properly solve some of these. If I see baseline 2 context, I can just guess.</p>

<p>Final study increases amounts of training data (x-axis) vs. performance (y).
ALBERT learning curve is quite steep compared to BERT/RoBERTa (Fig 2). Suggest
commonsense knowledge being useful (e.g. steel is hard, relative object sizes);
datasets should account for that with careful splits (how? Future work?).</p>

<h2 id="winogrande-debiasing-algorithm-aflite">Winogrande debiasing algorithm (AfLite)</h2>

<ol>
  <li>Finetune RoBERTa on a random subset of the data <code class="language-plaintext highlighter-rouge">D1</code></li>
  <li>Encode rest of the instances <code class="language-plaintext highlighter-rouge">D2</code></li>
  <li>Train linear classifiers <code class="language-plaintext highlighter-rouge">Li</code> on random subsets <code class="language-plaintext highlighter-rouge">D2_i</code></li>
  <li>If more than <code class="language-plaintext highlighter-rouge">k</code> linear <code class="language-plaintext highlighter-rouge">Li</code>s predict label correctly, discard.</li>
</ol>

<h2 id="training-details-and-performance">Training details and performance</h2>

<p>8 epochs (RTX 2080 for 13 (BERT), 14 (RoBERTa), 62 (ALBERT) min/epoch) on large
Winogrande train set &amp; models; Adam (LR of 1e-5); cross-entropy; batch size 8.</p>
<blockquote>
  <p>As the evaluation is conducted on the dev set, we do not use it to select the
best model. Instead, we report the performance with the final model, which is
converged <em>based on our observation</em>.</p>
</blockquote>


        </div>

        

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/probing-emergent-semantics-qa">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Probing Emergent Semantics in Predictive Agents via Question Answering

      </span>
    </a>
  

  
    <a class="page-next" href="/conformal-scoring">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        Conformal Classification of Neural Networks
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>

    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://twitter.com/tkukurin"><i class="fab fa-twitter-square fa-2x" title="Twitter"></i></a><a class="social-icon" href="https://github.com/tkukurin"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/toni/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a></div><div class="copyright">
    
      

    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>

</html>
