<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    <title>Object-based attention for spatio-temporal reasoning</title>
    <meta name="description" content="Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures. Evaluated on CLEVRER (CLEV...">
    <link rel="canonical" href="https://tkukurin.github.io/spatiotemporal-reasoning-dm">
  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/skins/light.css">
  <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

</head>


  <body class="layout--post  object-based-attention-for-spatio-temporal-reasoning">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/">~</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">caveat emptor</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <div class="page-sidebar">
        
            
            <h1 id="page-title" class="page-title p-name">Object-based attention for spatio-temporal reasoning
</h1>
        
        <div class="page-author h-card p-author"><div class="author-info">
    <time class="page-date dt-published" datetime="2021-01-17T00:00:00+00:00"><a class="u-url" href="">Jan 17, 2021</a>
</time>

  </div>
</div>

        

        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy">rl</li><li class="page-taxonomy">research</li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <p><a href="https://arxiv.org/pdf/2012.08508.pdf">Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures</a>.
Evaluated on <a href="https://mitibmwatsonailab.mit.edu/research/blog/clevrer-the-first-video-dataset-for-neuro-symbolic-reasoning/">CLEVRER</a>
(<a href="https://cs.stanford.edu/people/jcjohns/clevr/">CLEVR</a>, but for videos and with
causal questions - think “what would happen if …”)
and <a href="https://rohitgirdhar.github.io/CATER/">CATER</a> (CLEVR, but tracking object
permanence).</p>

<p>Builds on attention models (transformer obv). Paper highlights:</p>
<ul>
  <li>Self-attention to integrate information over time (<em>isn’t that a trend</em>).</li>
  <li>Input soft-discretization at the right level of abstraction (“pixels too fine,
entire scenes too coarse”)</li>
  <li>Self-supervised learning</li>
</ul>

<p>Biological research suggests that humans reason at the level of objects (i.e.
inferotemporal instead of V1 representations).</p>

<h2 id="model">Model</h2>

<p>Input to the model are sequential visual observations (video) and text (question).</p>

<p>They use MONet to obtain discrete objects from <strong>raw visual observations</strong>.
RNN obtains \(N_o\) attn masks (\(A_{ti} \in [0,1]^{w \times h}\) where
\(\sum A_{ti} = 1\)),
each representing <em>probability</em> that a given pixel belongs to that mask’s object.
Thus \(A_{ti} \odot F_t\) retrieves \(i\)th object on the image.
This value is encoded using a VAE, where latent posterior \(q(\mathbf{z}_{ti}|F_t, A_{ti})\)
is a Gaussian around \(\mu_{ti} \in \mathbb R^d\) (the \(i\)th object’s representation).</p>

<p><strong>Text input</strong> (question about the video) is represented as \(w_i \in \mathbb R^d\).</p>

<p>Additional one-hot value indicates whether input is a word or object (i.e. both
inputs are actually \(\mathbb R^{d+2}\)).</p>

<p>These values are <strong>jointly encoded</strong> as transformer input
\(\textrm{CLS}; m_{ti}\mu_{ti}|_{t,i}; w_i|_i\) where \(m_{ti}\) is a mask in \(\{0,1\}\).
As with BERT, the output of \(\textrm{CLS}\) is transformed through MLP (1
hidden layer \(N_H\)) in order to supervisedly learn the answer.</p>

<h2 id="training-procedure">Training procedure</h2>

<p>Two separate masking procedures.
One is standard BERT.
The other is <em>hierarchical</em>: first transformer outputs a vector \(r_o\) per frame
(output produced by attending to each <strong>object</strong> within the frame, along with
corresponding <strong>question text</strong>).
Individual object representations are then concatenated: \(R_t=(r_o; \ldots; r_O)\).
This serves as input for the next transformer in the hierarchy.</p>

<p><strong>Self-supervision</strong> tested on contrastive and L2 loss, with similar performance.
Detailed ablations left for future work.</p>

<h2 id="results">Results</h2>

<p>Outperforms previous SOTA.
No surprises: ablation study suggests self-attention is key to performance.
Global self-attention better than hierarchical.</p>

<p>Seems to require 40% less training data compared to neuro-symbolic models.
Self-supervised auxilliary loss highlighted as particularly important.</p>


        </div>

        

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/go-explore">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Go Explore

      </span>
    </a>
  

  
    <a class="page-next" href="/predicting-transferability-nlp">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        Predicting Transferability in NLP
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>

    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://twitter.com/tkukurin"><i class="fab fa-twitter-square fa-2x" title="Twitter"></i></a><a class="social-icon" href="https://github.com/tkukurin"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/toni/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a></div><div class="copyright">
    
      

    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>

</html>
