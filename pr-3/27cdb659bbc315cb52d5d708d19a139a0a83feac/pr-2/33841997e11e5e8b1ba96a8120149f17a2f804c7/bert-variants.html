<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    <title>BERT objectives</title>
    <meta name="description" content="A comprehensive up-to-date list of BERT variants (as well as other transformers) can be found on HuggingFace website. Below are just some of the different ob...">
    <link rel="canonical" href="https://tkukurin.github.io/bert-variants">
  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/skins/light.css">
  <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

</head>


  <body class="layout--post  bert-objectives">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/">~</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">caveat emptor</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <div class="page-sidebar">
        
            
            <h1 id="page-title" class="page-title p-name">BERT objectives
</h1>
        
        <div class="page-author h-card p-author"><div class="author-info">
    <time class="page-date dt-published" datetime="2021-02-24T00:00:00+00:00"><a class="u-url" href="">Feb 24, 2021</a>
</time>

  </div>
</div>

        

        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy">nlp</li><li class="page-taxonomy">transformers</li><li class="page-taxonomy">research</li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <p>A comprehensive up-to-date list of BERT variants (as well as other transformers)
can be found on <a href="https://huggingface.co/transformers/">HuggingFace website</a>.
Below are just some of the different objectives introduced in these papers.</p>

<h2 id="og-bert"><a href="https://arxiv.org/pdf/1810.04805.pdf">OG BERT</a></h2>

<p>The original BERT paper introduced pretraining using transformers in NLP.
Also continued the trend of doing <em>PR by naming</em> in ML research.</p>

<h3 id="masked-language-modeling-mlm--cloze">Masked Language Modeling (MLM / Cloze)</h3>

<p>WordPiece tokens chosen for prediction at random with 15% probability.
Input replaced with <code class="language-plaintext highlighter-rouge">[MASK]</code> 80% of the time, random other token 10%, and
unchanged remaining 10% of the time.</p>

<p>BERT is trained using cross-entropy loss between the masked token and a softmax
head producing predictions from its ~30k WordPiece vocabulary.</p>

<h3 id="next-sentence-prediction-nsp">Next Sentence Prediction (NSP)</h3>

<p>As input, encode either the correct next sentence after <code class="language-plaintext highlighter-rouge">[SEP]</code>, or a random
other (50%).
<code class="language-plaintext highlighter-rouge">[CLS]</code> predicts a binary outcome (whether the next sentence is correct).
Model achieves 98% accuracy.</p>

<p>In this paper, they indicate NSP is beneficial for QA and NLI; however,
subsequent works find the impact unreliable and often remove it.</p>

<h2 id="roberta"><a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa</a></h2>

<p><em>Robustly optimized BERT approach</em>.
Larger byte-level BPE; no NSP; larger mini-batches.
<em>Dynamic token masking.</em>
Investigates the pretraining dataset.</p>

<p><a href="https://arxiv.org/pdf/1911.03894.pdf">CamemBERT</a>
Builds on RoBERTa; no NSP, SentencePiece subword tokens for French, but uses
Whole Word Masking (WWM), i.e. <code class="language-plaintext highlighter-rouge">[MASK]</code> replaces entire words.</p>

<h3 id="full-sentences-fs-and-doc-sentences-ds-objectives">Full-sentences (FS) and Doc-sentences (DS) objectives</h3>

<p>Just concatenate as many sentences as you can from multiple documents (FS) or a
single document (DS), using <code class="language-plaintext highlighter-rouge">[SEP]</code> as delimiter. No NSP loss, 512 tokens max.
Example: <code class="language-plaintext highlighter-rouge">[CLS] A [SEP] B [SEP] C ...</code>.
DS works a bit better but FS was easier to implement so they use the latter.</p>

<h2 id="albert"><a href="https://arxiv.org/pdf/1909.11942.pdf">AlBERT</a></h2>

<p>Replaces NSP with a similar objective, aiming to teach <em>coherence</em>.</p>

<h3 id="sentence-ordering-so">Sentence Ordering (SO)</h3>

<p>It seems that the order of sentences is a more difficult prediction task.
They replace NSP with the SO objective, i.e. negative examples are sentences in
swapped order. A training example: <code class="language-plaintext highlighter-rouge">[CLS] A [SEP] B</code> and <code class="language-plaintext highlighter-rouge">[CLS] B [SEP] A</code>.</p>

<h2 id="bart"><a href="https://arxiv.org/pdf/1910.13461.pdf">BART</a></h2>

<p>Denoising autoencoder maps corrupted document to the original document.
Seq2seq with bidirectional model over corrupted text and left-to-right
autoregressive decoder.</p>

<p>The training procedure is kind of interesting as it’s different from classic BERT.
<a href="https://arxiv.org/pdf/2010.12321.pdf">BARThez</a> uses the same, except for French.</p>

<h3 id="document-corruption">Document corruption</h3>

<p>Input documents are corrupted using</p>
<ul>
  <li>Document rotation (shift)</li>
  <li>Sentence permutation</li>
  <li>Text infilling</li>
  <li>Token deletion</li>
  <li>Token masking</li>
</ul>

<p>The decoder then tries to recover entire uncorrupted input.</p>

<h2 id="other">Other</h2>

<p>A lot of the other Transformers work utilizes some variant of the objectives
above, while attempting to reduce the computational demands (squared in input
size).</p>

<p>E.g. <a href="https://arxiv.org/pdf/2008.02496.pdf">ConvBERT</a>
tries span-based dynamic convolutions to reduce global memory footprint.
Essentially localizes attention, since BERT heads frequently exhibit a diagonal
attention matrix anyway (cf. <a href="https://lena-voita.github.io/posts/acl19_heads.html">Lena Voita’s work</a>).</p>

<p><a href="https://arxiv.org/pdf/2010.10499.pdf">This work</a>
uses neural architecture search and Knowledge Distillation (KD).
KD seems to be a superior training objective to standard self-supervised in
terms of training speed (Fig. 1 in the paper).</p>


        </div>

        

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/learning-salon-recap-2020">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Learning Salon: Recap and Panel Discussion

      </span>
    </a>
  

  
    <a class="page-next" href="/learning-salon-katja-hofmann">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        Learning Salon: Katja Hofmann, Deep RL for Games
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>

    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://twitter.com/tkukurin"><i class="fab fa-twitter-square fa-2x" title="Twitter"></i></a><a class="social-icon" href="https://github.com/tkukurin"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/toni/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a></div><div class="copyright">
    
      

    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>

</html>
