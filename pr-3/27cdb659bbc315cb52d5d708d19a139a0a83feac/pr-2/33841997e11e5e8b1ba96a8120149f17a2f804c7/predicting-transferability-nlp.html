<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    <title>Predicting Transferability in NLP</title>
    <meta name="description" content="Work on transferability. Goal: learn to rank most transferable source tasks for some target task. Conclusions: transfer learning works.">
    <link rel="canonical" href="https://tkukurin.github.io/predicting-transferability-nlp">
  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/skins/light.css">
  <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

</head>


  <body class="layout--post  predicting-transferability-in-nlp">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/">~</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">caveat emptor</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <div class="page-sidebar">
        
            
            <h1 id="page-title" class="page-title p-name">Predicting Transferability in NLP
</h1>
        
        <div class="page-author h-card p-author"><div class="author-info">
    <time class="page-date dt-published" datetime="2021-01-30T00:00:00+00:00"><a class="u-url" href="">Jan 30, 2021</a>
</time>

  </div>
</div>

        

        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy">nlp</li><li class="page-taxonomy">research</li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <p><a href="https://arxiv.org/pdf/2005.00770.pdf">Work on transferability</a>.
Goal: learn to rank <em>most transferable</em> source tasks for some target task.
Conclusions: transfer learning works.</p>

<ul>
  <li>transfer gains are possible even with small source datasets</li>
  <li>out-of-class transfer (different source and target tasks) surprisingly works
(though not as well as in-class)</li>
  <li>similarity between source and target tasks matters more in low-data regimes</li>
</ul>

<h2 id="tasks">Tasks</h2>

<p>33 different NLP tasks evaluated using pretrained BERT; broadly categorized as:</p>

<ul>
  <li>Text classification / regression (CR)</li>
  <li>Question answering (QA)</li>
  <li>Sequence Labeling (SL)</li>
</ul>

<p>A pre-trained BERT is first fine-tuned on an intermediate <em>source</em> task, and
then fine-tuned again on a <em>target</em> task (in limited, i.e. 1k random-sampled, or
full data regime). Reported mean of 20 random restarts for each experiment.</p>

<p><strong>Relative transfer gain</strong> is defined as
\(g_{s\rightarrow t} = \frac{p_{s\rightarrow t} - p_t}{p_t}\).</p>

<h2 id="task-embedding-methods">Task embedding methods</h2>

<p><strong>DataSize (baseline):</strong>
Just rank by pre-training dataset size.</p>

<p><strong>CurveGrad (baseline):</strong>
Based on gradients of BERTâ€™s loss curve.
Assumption that multi-task will work better if main task quickly plateaus
(small) while aux. task improves (large negative gradients).
They compute grad at 10, 20, 30, 50, 70% while fine-tuning on 10k samples.
Rank source tasks descendingly using reciprocal rank fusion algorithm.</p>

<p><strong>TextEmb:</strong>
Average pooling of average token-level BERT embeddings \(h_x\) across an entire
dataset, \(\sum_{x \in D} \frac{h_x}{\|D\|}\).</p>

<p><strong>TaskEmb:</strong>
<a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher information matrix</a>.
Takes outputs into account, similar to <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Achille_Task2Vec_Task_Embedding_for_Meta-Learning_ICCV_2019_paper.pdf">Task2vec</a>.
Train, then feed entire training dataset through BERT and compute <em>empirical
Fisher</em> on feature activations:</p>

\[F_\theta(D) = \frac{1}{n} \sum_{i=1}^{n} [\nabla_{\theta} \log P_{\theta}(y^i|x^i) \nabla_{\theta} \log P_{\theta}(y^i|x^i)^T]\]

<h2 id="evaluation">Evaluation</h2>

<p>There is one <em>source</em> task which performs best for a given <em>target</em> task.
They evaluate source tasks using NDCG and average rank as measures.</p>

<p>For Normalized Discounted Cumulative Gain (<strong>NDCG</strong>, information retrieval
measure that evaluates the quality of the entire ranking, not just the rank of
the best source task), define \(\textrm{rel}_i\) to be the relevance
(target performance) of source task with rank \(i\) in the evaluated ranking
\(R\). Then (if \(p\) is the number of source tasks):</p>

\[\textrm{NDCG}_p = \frac{\textrm{DCG}_p(R_{\textrm{pred}})}{\textrm{DCG}_p(R_{\textrm{true}})}\]

\[\textrm{DCG}_p(R) = \sum_{i=1}^{p} \frac{2^{\textrm{rel}_i} - 1}{\log_2(i+1)}\]

<h2 id="conclusion">Conclusion</h2>

<p>Large datasets (MNLI, SNLI, SQuAD-2) often the best source tasks.
TaskEmb <em>generally</em> selects source tasks that yield positive transfer, and often
selects the best source task.
Take a look at
<a href="https://arxiv.org/pdf/2005.00770.pdf">some of the figures (pg. 4)</a>
for more detailed interesting findings.</p>


        </div>

        

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/spatiotemporal-reasoning-dm">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Object-based attention for spatio-temporal reasoning

      </span>
    </a>
  

  
    <a class="page-next" href="/symbolic-behavior-ai">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        Symbolic Behavior in AI
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>

    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://twitter.com/tkukurin"><i class="fab fa-twitter-square fa-2x" title="Twitter"></i></a><a class="social-icon" href="https://github.com/tkukurin"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/toni/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a></div><div class="copyright">
    
      

    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>

</html>
