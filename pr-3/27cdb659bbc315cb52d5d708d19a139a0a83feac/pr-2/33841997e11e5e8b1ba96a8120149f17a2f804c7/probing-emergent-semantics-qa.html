<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    <title>Probing Emergent Semantics in Predictive Agents via Question Answering</title>
    <meta name="description" content="Language Grounded (3D environment) Question Answering. Agents optimize an exploration objective with/out self-supervised losses; only the latter reliably cap...">
    <link rel="canonical" href="https://tkukurin.github.io/probing-emergent-semantics-qa">
  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/skins/light.css">
  <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

</head>


  <body class="layout--post  probing-emergent-semantics-in-predictive-agents-via-question-answering">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/">~</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">caveat emptor</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <div class="page-sidebar">
        
            
            <h1 id="page-title" class="page-title p-name">Probing Emergent Semantics in Predictive Agents via Question Answering
</h1>
        
        <div class="page-author h-card p-author"><div class="author-info">
    <time class="page-date dt-published" datetime="2021-10-09T00:00:00+00:00"><a class="u-url" href="">Oct 9, 2021</a>
</time>

  </div>
</div>

        

        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy">rl</li><li class="page-taxonomy">nlp</li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <p><a href="https://arxiv.org/pdf/2006.01016.pdf">Language Grounded (3D environment) Question Answering</a>.
Agents optimize an exploration objective with/out self-supervised losses; only
the latter reliably capture propositional knowledge in internal states
(representations can be decoded as answers).
Generative <a href="https://arxiv.org/pdf/1906.09237.pdf">SimCore (pixel space)</a>,
works for QA, <a href="https://arxiv.org/pdf/1811.06407.pdf">contrastive CPC|A doesn’t</a>.
Gradient doesn’t propagate from question answers.
Claim to corroborate <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1">Elman’s findings</a>.</p>

<blockquote>
  <p>Our study is a small example of how language can be used as a channel to probe
and understand what exactly agents can learn from their environments.</p>
</blockquote>

<p><img src="assets/img/2021-10-09-emergent-semantics-fig1.png" alt="TL;DR of the environment" /></p>

<p>They query factual knowledge about the environment:</p>
<ul>
  <li>shapes and colors (‘What shape is the <code class="language-plaintext highlighter-rouge">&lt;color&gt;</code> object?’)</li>
  <li>counting (‘How many <code class="language-plaintext highlighter-rouge">&lt;color&gt;</code> objects are there?’)</li>
  <li>spatial relations (‘What is the color of the <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> near the <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code>?’)</li>
  <li>exhaustive search (‘Is there a <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code>?’)</li>
  <li>comparisons (‘Are there the same number of <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> as <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code>?’)</li>
</ul>

<p>Test set aims for compositional knowledge (unseen object combinations / colors).</p>

<h2 id="related-work">Related work</h2>

<p>Some buzzwords:</p>
<ul>
  <li>Neural population decoding</li>
  <li>EmbodiedQA and grounded language learning</li>
  <li>Predictive modeling</li>
</ul>

<p>Delta from EmbodiedQA:</p>
<blockquote>
  <p>Typical approaches to EmbodiedQA involve training agents to move <em>for the goal
of answering questions</em>. In contrast, our focus is on learning a predictive
model in a <em>goal-agnostic exploration</em> phase and using question-answering as a
<em>post-hoc testbed</em> for evaluating the semantic knowledge that emerges in the
agent’s representations from predicting the future.</p>
</blockquote>

<p>Delta from other vision datasets:</p>
<blockquote>
  <p>Unlike the fully-observable setting in CLEVR, the agent does not get a global
view of the environment, and must answer these questions from a sequence of
partial egocentric observations.</p>
</blockquote>

<h2 id="environment">Environment</h2>

<p>The (now standard?) DeepMind L-shaped environment (see fig. 1);
50 different objects (shapes) x 10 colors;
input is a 96 × 72 1st-person RGB.</p>

<p>Actions (<a href="https://arxiv.org/pdf/2006.01016.pdf">cf. Table 5 for details</a>):</p>
<ul>
  <li>movements (move-{forward,back,left,right}),</li>
  <li>turns (turn-{up,down,left,right}),</li>
  <li>object pickup and manipulation (4 DoF: yaw, pitch, roll, and movement
along the axis between the agent and object).</li>
</ul>

<h2 id="model-and-reward">Model and reward</h2>

<p>CNN to produce latent vectors; LSTM to produce actions; IMPALA to train.
Exploration facilitated by rewarding the agent on approaching novel objects.</p>

<blockquote>
  <p>After visiting all objects, rewards are refreshed and available to be consumed
by the agent again (i.e.  re-visiting an object the agent has already been to
will now again lead to a +1.0 reward), and this process continues for the
duration of each episode (30 seconds or 900 steps).</p>
</blockquote>

<p>The QA decoder is another LSTM initiated from action-LSTM’s hidden state.
Trained via cross-entropy, gradients are not propagated to action-LSTM.</p>

<blockquote>
  <p>unrolled for a fixed number of computation steps after which it predicts a
softmax distribution over the vocabulary of one-word answers.</p>
</blockquote>

<p><img src="assets/img/2021-10-09-emergent-semantics-fig2.png" alt="TL;DR of the architecture" /></p>


        </div>

        

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/kg-based-world-models">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> KG-based world models

      </span>
    </a>
  

  
    <a class="page-next" href="/winograd-transformers">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>

    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://twitter.com/tkukurin"><i class="fab fa-twitter-square fa-2x" title="Twitter"></i></a><a class="social-icon" href="https://github.com/tkukurin"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/toni/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a></div><div class="copyright">
    
      

    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>

</html>
